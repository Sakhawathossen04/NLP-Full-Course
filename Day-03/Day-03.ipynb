{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2bab5ad-4abc-4c40-9015-c5332dc1e036",
   "metadata": {},
   "source": [
    "# Text Preprocessing in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb449e-4f33-4635-bee5-5a5900e03405",
   "metadata": {},
   "source": [
    "Text preprocessing is a crucial step in Natural Language Processing (NLP) that involves cleaning and transforming raw text data into a format suitable for analysis and machine learning models. This process is vital for enhancing the performance and accuracy of NLP tasks.\n",
    "\n",
    "One key reason for text preprocessing is to remove noise and irrelevant information from the text, such as special characters, punctuation, and stop words. This helps in reducing the dimensionality of the data and improves the efficiency of subsequent analysis. Additionally, text normalization techniques, such as stemming and lemmatization, ensure that words are represented in their base or root form, reducing redundancy and enhancing the consistency of the dataset.\n",
    "\n",
    "For example, consider the sentence: \"The quick brown foxes are jumping over the lazy dogs.\" After preprocessing, it might become: \"quick brown fox jump lazy dog.\" This simplification facilitates better feature extraction and enables NLP models to focus on the essential linguistic elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf4ec8b-1670-4519-9ab3-b684a2a2875a",
   "metadata": {},
   "source": [
    "Moreover, text preprocessing addresses issues like :\n",
    "\n",
    "1.Lowercase letters.\n",
    "\n",
    "2.Removing HTML tags.\n",
    "\n",
    "3.Removing URLs.\n",
    "\n",
    "4.Removing punctuation.\n",
    "\n",
    "5.Chat Words Treatment.\n",
    "\n",
    "6.Spelling Correction.\n",
    "\n",
    "7.Removing stop words\n",
    "\n",
    "8.Handling Emojies\n",
    "\n",
    "9.Tokenization\n",
    "\n",
    "10.Stemming\n",
    "\n",
    "11.Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78d3ba5-0889-4b33-a5fc-34a41b0efd9d",
   "metadata": {},
   "source": [
    "# Dataset Used in This Project Will be IMBD Movies Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "543b082b-f199-409d-bdd5-0d994985f862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8feffcab-6c7a-408f-851a-12fdda7a8d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da546d6a-b8be-4a1d-83e0-bccaea18be6f",
   "metadata": {},
   "source": [
    "# 1.Lowercase letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c565d6-1136-47d0-b6e6-aa62ea8add29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        One of the other reviewers has mentioned that ...\n",
       "1        A wonderful little production. <br /><br />The...\n",
       "2        I thought this was a wonderful way to spend ti...\n",
       "3        Basically there's a family where a little boy ...\n",
       "4        Petter Mattei's \"Love in the Time of Money\" is...\n",
       "                               ...                        \n",
       "49995    I thought this movie did a down right good job...\n",
       "49996    Bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    I am a Catholic taught in parochial elementary...\n",
       "49998    I'm going to have to disagree with the previou...\n",
       "49999    No one expects the Star Trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f55837db-6b7c-4b22-9c64-266a035a4e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I\\'d laughed at one of Woody\\'s comedies in years (dare I say a decade?). While I\\'ve never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b4c9cca-523b-49b3-85a8-01f69a43eed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. the plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). while some may be disappointed when they realize this is not match point 2: risk addiction, i thought it was proof that woody allen is still fully in control of the style many of us have grown to love.<br /><br />this was the most i\\'d laughed at one of woody\\'s comedies in years (dare i say a decade?). while i\\'ve never been impressed with scarlet johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />this may not be the crown jewel of his career, but it was wittier than \"devil wears prada\" and more interesting than \"superman\" a great comedy to go see with friends.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][2].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89380da-cf34-434a-a43a-68b2216e36ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you wannto lower fill dataset then you can write "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35367381-e793-486d-ae7a-9b45d76c235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d98fad2f-ce55-4903-a270-400a9a8fb7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    one of the other reviewers has mentioned that ...\n",
       "1    a wonderful little production. <br /><br />the...\n",
       "2    i thought this was a wonderful way to spend ti...\n",
       "3    basically there's a family where a little boy ...\n",
       "4    petter mattei's \"love in the time of money\" is...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0ac807-7692-41a3-b691-529c68fcc8ec",
   "metadata": {},
   "source": [
    "# 2.Removing HTML tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7086baa-38be-4a45-939c-8f62a4f675c2",
   "metadata": {},
   "source": [
    "Removing HTML tags is an essential step in NLP text preprocessing to ensure that only meaningful textual content is analyzed. HTML tags contain formatting information and metadata irrelevant to linguistic analysis. Including these tags can introduce noise and distort the analysis results. Removing HTML tags helps to extract pure textual data, making it easier to focus on the actual content of the text. This step is particularly crucial when dealing with web data or documents containing HTML markup, as it ensures that the extracted text accurately represents the intended linguistic information for NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d66349-09a6-4452-9311-104d3af40709",
   "metadata": {},
   "source": [
    "We can simply remove HTML tags by using the Regular Expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "382fbd4e-e651-45a8-9551-55c4379f5b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# how is it work? if you dont understand dont worry . search it google / regexp website and make pattern on your suitable text . if you wanna learn whole regexp the you see any vedio from yt.\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e3cd386-411b-41f2-90d5-7e92e2d61261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppose we have a text Which Contains HTML Tags \n",
    "text = \"<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b4b5031-e727-4952-bd48-a09663bafab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Movie 1 Actor - Aamir Khan Click here to download'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_html_tags(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feadead-d7b5-40f0-a47e-a8b8cc5cba9a",
   "metadata": {},
   "source": [
    "See How the Code perform well and clean the text from the HTML Tags , We can Also Apply this Function to Whole Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a24279b-50bc-419a-9aba-483be7e29390",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']= df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fd1953a-cd8d-46c9-b034-f09dbec10ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    one of the other reviewers has mentioned that ...\n",
       "1    a wonderful little production. the filming tec...\n",
       "2    i thought this was a wonderful way to spend ti...\n",
       "3    basically there's a family where a little boy ...\n",
       "4    petter mattei's \"love in the time of money\" is...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58039e8-c89a-4b56-bb02-ea085543aea6",
   "metadata": {},
   "source": [
    "# 3.Removing URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff827bca-72c3-4f30-8521-172d609a5eb2",
   "metadata": {},
   "source": [
    "In NLP text preprocessing, removing URLs is essential to eliminate irrelevant information that doesn't contribute to linguistic analysis. URLs contain website addresses, hyperlinks, and other web-specific elements that can skew the analysis and confuse machine learning models. By removing URLs, the focus remains on the textual content relevant to the task at hand, enhancing the accuracy of NLP tasks such as sentiment analysis, text classification, and information extraction. This step streamlines the dataset, reduces noise, and ensures that the model's attention is directed towards meaningful linguistic patterns and structures within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b972eb5-8183-404f-88ce-3077996fd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here We also Use Regular Expressions to Remove URLs from Text or Whole Corpus.\n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea84cf8d-d06d-4c59-8223-99895ad28dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we have the FOllowings Text With URL.\n",
    "text1 = 'Check out my notebook https://notebook8223fc1abb'\n",
    "text2 = 'Check out my notebook http://notebook8223fc1abb'\n",
    "text3 = 'Google search here www.google.com'\n",
    "text4 = 'For notebook click https:c1abb to search check www.google.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c65a620-e275-4d6c-8f1a-52196940427d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out my notebook \n",
      "Check out my notebook \n",
      "Google search here \n",
      "For notebook click https:c1abb to search check \n"
     ]
    }
   ],
   "source": [
    "# Lets Remove The URL by Calling Function\n",
    "print(remove_url(text1))\n",
    "print(remove_url(text2))\n",
    "print(remove_url(text3))\n",
    "print(remove_url(text4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d4d26-360a-42c5-bf48-9a1eb7ffca2a",
   "metadata": {},
   "source": [
    "# 4. Remove Punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae0168-5c31-4682-9e06-24c1c3037faa",
   "metadata": {},
   "source": [
    "Removing punctuation marks is essential in NLP text preprocessing to enhance the accuracy and efficiency of analysis. Punctuation marks like commas, periods, and quotation marks carry little semantic meaning and can introduce noise into the dataset. By removing them, the text becomes cleaner and more uniform, making it easier for machine learning models to extract meaningful features and patterns. Additionally, removing punctuation aids in standardizing the text, ensuring consistency across documents and improving the overall performance of NLP tasks such as sentiment analysis, text classification, and named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "231fa670-d768-416a-bc27-5a3014295bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From string we import Punctuation\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1cde3e2-6155-41a4-b134-8c91aae1fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261481aa-a497-4cbb-9aad-80e4f039faf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1f984-92e9-4d2b-be00-c03133c85291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ccea83d-45ac-4363-8cdc-5772fbf16663",
   "metadata": {},
   "source": [
    "The code defines a function \"remove_punc\"  that takes a text input and removes all punctutaion characters from it using.\n",
    "\n",
    "The translate method crate a translation table by str.maketrans . This function effiectivley cleanses the text of punctuation symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be96278b-80fd-4e6b-b465-3dc2920f768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('','',punc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05c6d5a8-24ba-473f-aa3e-98c195cfbb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The quick brown fox jumps over the lazy dog. However, the dog doesn't seem impressed! Oh no, it just yawned. How disappointing! Maybe a squirrel would elicit a reaction. Alas, the fox is out of luck.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog. However, the dog doesn't seem impressed! Oh no, it just yawned. How disappointing! Maybe a squirrel would elicit a reaction. Alas, the fox is out of luck.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1073fff3-4f05-4f4b-998a-bd31c4b882c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog However the dog doesnt seem impressed Oh no it just yawned How disappointing Maybe a squirrel would elicit a reaction Alas the fox is out of luck'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Punctuation.\n",
    "remove_punc(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca58889d-0504-4c27-aeaa-18b356911d8e",
   "metadata": {},
   "source": [
    "Hence the function removes the punctuations from the text and we can also use this function to remove the punctuations from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89c38c45-371c-4c4f-99a3-2d315f7f63ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\n"
     ]
    }
   ],
   "source": [
    "print(df['review'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f09df465-5fa5-4d01-ba82-91aa3064c233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Basically theres a family where a little boy Jake thinks theres a zombie in his closet  his parents are fighting all the timebr br This movie is slower than a soap opera and suddenly Jake decides to become Rambo and kill the zombiebr br OK first of all when youre going to make a film you must Decide if its a thriller or a drama As a drama the movie is watchable Parents are divorcing  arguing like in real life And then we have Jake with his closet which totally ruins all the film I expected to see a BOOGEYMAN similar movie and instead i watched a drama with some meaningless thriller spotsbr br 3 out of 10 just for the well playing parents  descent dialogs As for the shots with Jake just ignore them'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punc(df['review'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d435440-1ab8-4f61-8bcc-b04da0dbe23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here look the html tag are under the punc ,so it also remove "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be29ead-4056-467c-ae1a-eeabfbf63dde",
   "metadata": {},
   "source": [
    "# 5. Handling ChatWords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bef44a4-045d-4912-ba36-9e6b429307b5",
   "metadata": {},
   "source": [
    "Handling ChatWords, also known as internet slang or informal language used in online communication, is important in NLP text preprocessing to ensure accurate analysis and understanding of text data. By converting ChatWords into their standard English equivalents or formal language equivalents, NLP models can effectively interpret the meaning of the text. This preprocessing step helps in maintaining consistency, improving the quality of input data, and enhancing the performance of NLP tasks such as sentiment analysis, chatbots, and information retrieval systems. Ultimately, handling ChatWords ensures better comprehension and more reliable results in NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8908bf2-f326-4372-be43-dffccbc93191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here Come ChatWords Which i Get from a Github Repository\n",
    "# Repository Link : https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt\n",
    "chat_words = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
    "    \"ILU\": \"ILU: I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A.. Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA?\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\",\n",
    "    \"TFW\": \"That feeling when\",\n",
    "    \"MFW\": \"My face when\",\n",
    "    \"MRW\": \"My reaction when\",\n",
    "    \"IFYP\": \"I feel your pain\",\n",
    "    \"TNTL\": \"Trying not to laugh\",\n",
    "    \"JK\": \"Just kidding\",\n",
    "    \"IDC\": \"I don't care\",\n",
    "    \"ILY\": \"I love you\",\n",
    "    \"IMU\": \"I miss you\",\n",
    "    \"ADIH\": \"Another day in hell\",\n",
    "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
    "    \"WYWH\": \"Wish you were here\",\n",
    "    \"TIME\": \"Tears in my eyes\",\n",
    "    \"BAE\": \"Before anyone else\",\n",
    "    \"FIMH\": \"Forever in my heart\",\n",
    "    \"BSAAW\": \"Big smile and a wink\",\n",
    "    \"BWL\": \"Bursting with laughter\",\n",
    "    \"BFF\": \"Best friends forever\",\n",
    "    \"CSL\": \"Can't stop laughing\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c6645-7622-4fec-9f8a-c7933951bff6",
   "metadata": {},
   "source": [
    "The code defines a function, chat_conversion, that replaces text with their corresponding chat acronyms from a predefined dictionary. It iterates through each word in the input text, checks if it exists in the dictionary, and replaces it if found. The modified text is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "066c07ed-beb2-48df-a10c-6b91f9545e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In My Honest/Humble Opinion'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_words['IMHO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1add9298-cb6c-4bfb-a62b-ca2230d2eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text =[]\n",
    "    for i in text.split():\n",
    "        if i.upper() in chat_words:\n",
    "            new_text.append(chat_words[i.upper()])\n",
    "        else:\n",
    "            new_text.append(i)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03878c97-6275-4c92-b55d-eaa632d18c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In My Honest/Humble Opinion he is the best\n",
      "For Your Information Islamabad is the capital of Pakistan\n"
     ]
    }
   ],
   "source": [
    "text = 'IMHO he is the best'\n",
    "text1 = 'FYI Islamabad is the capital of Pakistan'\n",
    "# Calling function\n",
    "print(chat_conversion(text))\n",
    "print(chat_conversion(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383db2d9-d1d4-4900-8546-2810dd59cf30",
   "metadata": {},
   "source": [
    "Well this is how we Handle ChatWords in Our Data Simple u have to call the above Function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55b3f0-ea9a-430c-87f9-bbb594f904bb",
   "metadata": {},
   "source": [
    "# 6. Spelling Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecd96dc-950c-4335-b4d8-5cd2014dcedc",
   "metadata": {},
   "source": [
    "Spelling correction is a crucial aspect of NLP text preprocessing to enhance data quality and improve model performance. It addresses errors in text caused by typographical mistakes, irregularities, or variations in spelling. Correcting spelling errors ensures consistency and accuracy in the dataset, reducing ambiguity and improving the reliability of NLP tasks like sentiment analysis, machine translation, and information retrieval. By standardizing spelling across the dataset, models can better understand and process text, leading to more precise and reliable results in natural language processing applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3113e48-27fb-4eba-a8ab-799872058aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import this library to handle the spelling issue.\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88c48f73-d92e-4ab1-8a0d-201da0ffb071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.\n",
      "The cat sat on the cuchion. while plyaiing\n"
     ]
    }
   ],
   "source": [
    "# Incorrect text\n",
    "text1 = 'ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.'\n",
    "print(text1)\n",
    "# Incorrect Text 2 \n",
    "text2 = 'The cat sat on the cuchion. while plyaiing'\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7406989e-052c-478e-8855-1ce94b159a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certain conditions during several generations are modified in the same manner.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calling function \n",
    "textBlb=TextBlob(text1)\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5187a1a2-9fdb-4d38-a55c-f4e51f4d1ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cat sat on the cushion. while playing'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textBlb1=TextBlob(text2)\n",
    "textBlb1.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e45b4-41de-4def-90b9-3e270bcac96f",
   "metadata": {},
   "source": [
    "# 7. Handling StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23194fe-2de7-4c87-b26b-666b42c96c9c",
   "metadata": {},
   "source": [
    "In NLP text preprocessing, removing stop words is crucial to enhance the quality and efficiency of analysis. Stop words are common words like \"the,\" \"is,\" and \"and,\" which appear frequently in text but carry little semantic meaning. By eliminating stop words, we reduce noise in the data, decrease the dimensionality of the dataset, and improve the accuracy of NLP tasks such as sentiment analysis, topic modeling, and text classification. This process streamlines the analysis by focusing on the significant words that carry more meaningful information, leading to better model performance and interpretation of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26a08ba9-384b-4faa-a56d-60835630241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use NLTK library to remove Stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "846a19df-8631-430d-9105-aa919eb686c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can see all the stopwords in English.However we can chose different Languages also like spanish etc.\n",
    "stopword = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ef193c7-6d12-49c8-b992-60956395e08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90bc9d87-351f-468c-8d1d-47a51aefa526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for  i in text.split():\n",
    "        if i in stopword:\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(i)\n",
    "\n",
    "    return \" \".join(new_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e64ed0b-d01c-4748-8cc7-9e434922f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text With Stop Words :probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it's not preachy or boring. it just never gets old, despite my having seen it some 15 or more times\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'probably  all-time favorite movie,  story  selflessness, sacrifice  dedication   noble cause,    preachy  boring.   never gets old, despite   seen   15   times'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text\n",
    "text = 'probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times'\n",
    "print(f'Text With Stop Words :{text}')\n",
    "# Calling Function\n",
    "remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a56c0d32-8ba6-4af4-b0d7-72c399eed10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        One    reviewers  mentioned   watching  1 Oz e...\n",
       "1        A wonderful little production. <br /><br />The...\n",
       "2        I thought    wonderful way  spend time    hot ...\n",
       "3        Basically there's  family   little boy (Jake) ...\n",
       "4        Petter Mattei's \"Love   Time  Money\"   visuall...\n",
       "                               ...                        \n",
       "49995    I thought  movie    right good job. It   creat...\n",
       "49996    Bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    I   Catholic taught  parochial elementary scho...\n",
       "49998    I'm going    disagree   previous comment  side...\n",
       "49999    No one expects  Star Trek movies   high art,  ...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can Apply the same Function on Whole Corpus also \n",
    "df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7a464-f752-4904-9cdc-896f7861751c",
   "metadata": {},
   "source": [
    "# 8. Handling Emojies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63de11b4-de83-4999-af7e-37329c343989",
   "metadata": {},
   "source": [
    "Handling emojis in NLP text preprocessing is essential for several reasons. Emojis convey valuable information about sentiment, emotion, and context in text data, especially in informal communication channels like social media. However, they pose challenges for NLP algorithms due to their non-textual nature. Preprocessing involves converting emojis into meaningful representations, such as replacing them with textual descriptions or mapping them to specific sentiment categories. By handling emojis effectively, NLP models can accurately interpret and analyze text data, leading to improved performance in sentiment analysis, emotion detection, and other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8c1e7f-c4df-48d6-aa9d-feed22db5180",
   "metadata": {},
   "source": [
    "8.1 Simply Remove Emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b13f96-226a-4d7b-9359-c8bfbccd1cc8",
   "metadata": {},
   "source": [
    "The code defines a function, remove_emoji, which uses a regular expression to match and remove all emojis from a given text string. It targets various Unicode ranges corresponding to different categories of emojis and replaces them with an empty string, effectively removing them from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8923c912-e31c-4420-9b88-1a22b79c65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again Here we use The Regular Expressions to Remove the Emojies from Text or Whole Corpus.\n",
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "05b23713-38f5-4955-be9a-02113855cc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was ðŸ˜˜ \n",
      " Python is ðŸ”¥\n"
     ]
    }
   ],
   "source": [
    "# Texts \n",
    "text = \"Loved the movie. It was ðŸ˜˜\"\n",
    "text1 = 'Python is ðŸ”¥'\n",
    "print(text ,'\\n', text1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f913f76-a4eb-44b8-8176-1c3f62a3d3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loved the movie. It was '"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Emojies using Fucntion\n",
    "remove_emoji(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "68a6af53-3b55-412c-b474-46ade34829c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python is '"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a931b27-50fd-4b69-88fe-e3681ad1b483",
   "metadata": {},
   "source": [
    "8.2 Simply Convert Emojis into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a897da5d-d0da-4a2d-8658-2d1779b5d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use emoji library \n",
    "#pip install emoji\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901f37ee-8ce7-4cf4-9f72-9dac26867669",
   "metadata": {},
   "source": [
    "Calling Emoji tools Demojize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad93d167-a79c-4e40-903b-aa9f083f2aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loved the movie. It was :face_blowing_a_kiss:'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.demojize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d058ea12-f1c3-4d2b-900e-46910862bafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python is :fire:'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.demojize(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0707e62-c7b5-459b-923f-3b19e3f55946",
   "metadata": {},
   "source": [
    "# 9. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4792fb-031f-4294-a25d-00364a9405a8",
   "metadata": {},
   "source": [
    "Tokenization is a crucial step in NLP text preprocessing where text is segmented into smaller units, typically words or subwords, known as tokens. This process is essential for several reasons. Firstly, it breaks down the text into manageable units for analysis and processing. Secondly, it standardizes the representation of words, enabling consistency in language modeling tasks. Additionally, tokenization forms the basis for feature extraction and modeling in NLP, facilitating tasks such as sentiment analysis, named entity recognition, and machine translation. Overall, tokenization plays a fundamental role in preparing text data for further analysis and modeling in NLP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194003c9-6f4a-4ef7-8c2c-eff900d6d3a8",
   "metadata": {},
   "source": [
    "We Generally do 2 Type of tokenization 1. Word tokenization 2. Sentence Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19648e57-678e-4088-94bb-ae2e4a8db01b",
   "metadata": {},
   "source": [
    "# 9.1 NLTK\n",
    "# NLTK is a Library used to tokenize text into sentences and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "72557b2a-1cc6-4c59-90c9-947d949f4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ba8fd702-6225-4b18-89c3-629cc326a6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'visit', 'dhaka', '!']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text\n",
    "sentence = 'I am going to visit dhaka!'\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "10ea8b0f-3395-41da-9026-4c996172435c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whole text Containing 2 or more Sentences\n",
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e8b9a924-638a-4fbd-b85a-6e9ff6d6356e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']\n",
      "['We', \"'re\", 'here', 'to', 'help', '!', 'mail', 'us', 'at', 'nks', '@', 'gmail.com']\n",
      "['A', '5km', 'ride', 'cost', '$', '10.50']\n"
     ]
    }
   ],
   "source": [
    "# Some Sentences \n",
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.50'\n",
    "\n",
    "# Word Tokenize the Sentences\n",
    "print(word_tokenize(sent5))\n",
    "print(word_tokenize(sent6))\n",
    "print(word_tokenize(sent7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85936966-e92d-45b8-bf82-5f63e04c43c1",
   "metadata": {},
   "source": [
    "NLTK is Performing Well Altough it has some of issue , Like in above text u see it cannot handle the mail. But U can Use it Acording to the data problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf3d40-caa0-4197-8f81-bbdf7ef2b878",
   "metadata": {},
   "source": [
    "# 9.1 Spacy\n",
    "# Spacy is a Library used to tokenize text into sentences and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5b3476f5-64ae-4610-807d-cdcf2d147457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.11-cp314-cp314-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.15-cp314-cp314-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.13-cp314-cp314-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.12-cp314-cp314-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.10-cp314-cp314-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.2-cp314-cp314-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer_slim-0.20.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from spacy) (2.3.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from spacy) (25.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.41.5-cp314-cp314-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.3-cp314-cp314-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading wrapt-2.0.1-cp314-cp314-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from jinja2->spacy) (3.0.3)\n",
      "Downloading spacy-3.8.11-cp314-cp314-win_amd64.whl (14.4 MB)\n",
      "   ---------------------------------------- 0.0/14.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.0/14.4 MB 7.0 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.3/14.4 MB 3.2 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.6/14.4 MB 2.8 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.8/14.4 MB 2.6 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 2.1/14.4 MB 2.1 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 3.1/14.4 MB 2.6 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.4/14.4 MB 2.3 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.9/14.4 MB 2.3 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.5/14.4 MB 2.3 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 5.0/14.4 MB 2.3 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 5.2/14.4 MB 2.3 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 5.5/14.4 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.8/14.4 MB 2.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.8/14.4 MB 2.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.8/14.4 MB 2.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 6.0/14.4 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 6.0/14.4 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 6.0/14.4 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 6.0/14.4 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 6.0/14.4 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 6.0/14.4 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 6.0/14.4 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 6.0/14.4 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 6.0/14.4 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 6.0/14.4 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 6.0/14.4 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 6.0/14.4 MB 1.8 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 6.3/14.4 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 6.3/14.4 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 6.3/14.4 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 6.3/14.4 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 6.3/14.4 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 6.3/14.4 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 6.3/14.4 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 6.3/14.4 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 6.3/14.4 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 6.3/14.4 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 6.3/14.4 MB 1.0 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 6.6/14.4 MB 788.1 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.6/14.4 MB 788.1 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.6/14.4 MB 788.1 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.6/14.4 MB 788.1 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.6/14.4 MB 788.1 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.6/14.4 MB 788.1 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.6/14.4 MB 788.1 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.6/14.4 MB 788.1 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.6/14.4 MB 788.1 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.6/14.4 MB 788.1 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.6/14.4 MB 788.1 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.6/14.4 MB 788.1 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.6/14.4 MB 788.1 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.6/14.4 MB 788.1 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.6/14.4 MB 788.1 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.6/14.4 MB 788.1 kB/s eta 0:00:10\n",
      "   ------------------ --------------------- 6.8/14.4 MB 571.0 kB/s eta 0:00:14\n",
      "   ------------------ --------------------- 6.8/14.4 MB 571.0 kB/s eta 0:00:14\n",
      "   ------------------ --------------------- 6.8/14.4 MB 571.0 kB/s eta 0:00:14\n",
      "   ------------------- -------------------- 7.1/14.4 MB 562.0 kB/s eta 0:00:14\n",
      "   ------------------- -------------------- 7.1/14.4 MB 562.0 kB/s eta 0:00:14\n",
      "   ------------------- -------------------- 7.1/14.4 MB 562.0 kB/s eta 0:00:14\n",
      "   -------------------- ------------------- 7.3/14.4 MB 563.9 kB/s eta 0:00:13\n",
      "   --------------------- ------------------ 7.6/14.4 MB 568.5 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 7.6/14.4 MB 568.5 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 7.9/14.4 MB 569.1 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 7.9/14.4 MB 569.1 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 7.9/14.4 MB 569.1 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 7.9/14.4 MB 569.1 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 7.9/14.4 MB 569.1 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 7.9/14.4 MB 569.1 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 7.9/14.4 MB 569.1 kB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 8.1/14.4 MB 530.7 kB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 8.1/14.4 MB 530.7 kB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 8.1/14.4 MB 530.7 kB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 8.1/14.4 MB 530.7 kB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 8.4/14.4 MB 522.7 kB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 8.4/14.4 MB 522.7 kB/s eta 0:00:12\n",
      "   ------------------------ --------------- 8.7/14.4 MB 523.0 kB/s eta 0:00:11\n",
      "   ------------------------ --------------- 8.7/14.4 MB 523.0 kB/s eta 0:00:11\n",
      "   ------------------------ --------------- 8.9/14.4 MB 526.4 kB/s eta 0:00:11\n",
      "   ------------------------- -------------- 9.2/14.4 MB 531.3 kB/s eta 0:00:10\n",
      "   ------------------------- -------------- 9.2/14.4 MB 531.3 kB/s eta 0:00:10\n",
      "   -------------------------- ------------- 9.4/14.4 MB 537.1 kB/s eta 0:00:10\n",
      "   -------------------------- ------------- 9.7/14.4 MB 542.5 kB/s eta 0:00:09\n",
      "   -------------------------- ------------- 9.7/14.4 MB 542.5 kB/s eta 0:00:09\n",
      "   --------------------------- ------------ 10.0/14.4 MB 548.0 kB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 10.2/14.4 MB 555.1 kB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 10.5/14.4 MB 562.7 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 10.7/14.4 MB 570.4 kB/s eta 0:00:07\n",
      "   ------------------------------ --------- 11.0/14.4 MB 578.0 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 11.3/14.4 MB 584.3 kB/s eta 0:00:06\n",
      "   -------------------------------- ------- 11.5/14.4 MB 591.8 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 11.8/14.4 MB 599.9 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 12.1/14.4 MB 608.2 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 12.6/14.4 MB 625.1 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 12.8/14.4 MB 633.7 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 13.1/14.4 MB 642.0 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 13.6/14.4 MB 657.4 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 13.9/14.4 MB 665.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  14.2/14.4 MB 674.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.4/14.4 MB 680.2 kB/s  0:00:20\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp314-cp314-win_amd64.whl (40 kB)\n",
      "Downloading murmurhash-1.0.15-cp314-cp314-win_amd64.whl (26 kB)\n",
      "Downloading preshed-3.0.12-cp314-cp314-win_amd64.whl (120 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp314-cp314-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 1.0/2.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 2.6 MB/s  0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp314-cp314-win_amd64.whl (658 kB)\n",
      "   ---------------------------------------- 0.0/658.9 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 524.3/658.9 kB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 658.9/658.9 kB 2.7 MB/s  0:00:00\n",
      "Downloading thinc-8.3.10-cp314-cp314-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.6/1.7 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 3.5 MB/s  0:00:00\n",
      "Downloading blis-1.3.3-cp314-cp314-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.0/6.3 MB 2.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.6/6.3 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.1/6.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.1/6.3 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.4/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 3.9/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.0/6.3 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 2.4 MB/s  0:00:02\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading typer_slim-0.20.1-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading wrapt-2.0.1-cp314-cp314-win_amd64.whl (60 kB)\n",
      "Installing collected packages: wrapt, wasabi, typing-inspection, spacy-loggers, spacy-legacy, pydantic-core, murmurhash, cymem, cloudpathlib, catalogue, blis, annotated-types, typer-slim, srsly, smart-open, pydantic, preshed, confection, weasel, thinc, spacy\n",
      "\n",
      "   ----------------------------------------  0/21 [wrapt]\n",
      "   ----------------------------------------  0/21 [wrapt]\n",
      "   ----------------------------------------  0/21 [wrapt]\n",
      "   - --------------------------------------  1/21 [wasabi]\n",
      "   - --------------------------------------  1/21 [wasabi]\n",
      "   - --------------------------------------  1/21 [wasabi]\n",
      "   - --------------------------------------  1/21 [wasabi]\n",
      "   --- ------------------------------------  2/21 [typing-inspection]\n",
      "   ----- ----------------------------------  3/21 [spacy-loggers]\n",
      "   ----- ----------------------------------  3/21 [spacy-loggers]\n",
      "   ----- ----------------------------------  3/21 [spacy-loggers]\n",
      "   ------- --------------------------------  4/21 [spacy-legacy]\n",
      "   ------- --------------------------------  4/21 [spacy-legacy]\n",
      "   ------- --------------------------------  4/21 [spacy-legacy]\n",
      "   ------- --------------------------------  4/21 [spacy-legacy]\n",
      "   ------- --------------------------------  4/21 [spacy-legacy]\n",
      "   --------- ------------------------------  5/21 [pydantic-core]\n",
      "   ----------- ----------------------------  6/21 [murmurhash]\n",
      "   ------------- --------------------------  7/21 [cymem]\n",
      "   --------------- ------------------------  8/21 [cloudpathlib]\n",
      "   --------------- ------------------------  8/21 [cloudpathlib]\n",
      "   --------------- ------------------------  8/21 [cloudpathlib]\n",
      "   --------------- ------------------------  8/21 [cloudpathlib]\n",
      "   --------------- ------------------------  8/21 [cloudpathlib]\n",
      "   --------------- ------------------------  8/21 [cloudpathlib]\n",
      "   --------------- ------------------------  8/21 [cloudpathlib]\n",
      "   --------------- ------------------------  8/21 [cloudpathlib]\n",
      "   ----------------- ----------------------  9/21 [catalogue]\n",
      "   ----------------- ----------------------  9/21 [catalogue]\n",
      "   ------------------- -------------------- 10/21 [blis]\n",
      "   ------------------- -------------------- 10/21 [blis]\n",
      "   ------------------- -------------------- 10/21 [blis]\n",
      "   -------------------- ------------------- 11/21 [annotated-types]\n",
      "   ---------------------- ----------------- 12/21 [typer-slim]\n",
      "   ---------------------- ----------------- 12/21 [typer-slim]\n",
      "   ---------------------- ----------------- 12/21 [typer-slim]\n",
      "   ---------------------- ----------------- 12/21 [typer-slim]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   ------------------------ --------------- 13/21 [srsly]\n",
      "   -------------------------- ------------- 14/21 [smart-open]\n",
      "   -------------------------- ------------- 14/21 [smart-open]\n",
      "   -------------------------- ------------- 14/21 [smart-open]\n",
      "   -------------------------- ------------- 14/21 [smart-open]\n",
      "   -------------------------- ------------- 14/21 [smart-open]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ---------------------------- ----------- 15/21 [pydantic]\n",
      "   ------------------------------ --------- 16/21 [preshed]\n",
      "   -------------------------------- ------- 17/21 [confection]\n",
      "   -------------------------------- ------- 17/21 [confection]\n",
      "   ---------------------------------- ----- 18/21 [weasel]\n",
      "   ---------------------------------- ----- 18/21 [weasel]\n",
      "   ---------------------------------- ----- 18/21 [weasel]\n",
      "   ---------------------------------- ----- 18/21 [weasel]\n",
      "   ---------------------------------- ----- 18/21 [weasel]\n",
      "   ---------------------------------- ----- 18/21 [weasel]\n",
      "   ---------------------------------- ----- 18/21 [weasel]\n",
      "   ---------------------------------- ----- 18/21 [weasel]\n",
      "   ---------------------------------- ----- 18/21 [weasel]\n",
      "   ---------------------------------- ----- 18/21 [weasel]\n",
      "   ---------------------------------- ----- 18/21 [weasel]\n",
      "   ---------------------------------- ----- 18/21 [weasel]\n",
      "   ---------------------------------- ----- 18/21 [weasel]\n",
      "   ---------------------------------- ----- 18/21 [weasel]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   ------------------------------------ --- 19/21 [thinc]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   -------------------------------------- - 20/21 [spacy]\n",
      "   ---------------------------------------- 21/21 [spacy]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 blis-1.3.3 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 murmurhash-1.0.15 preshed-3.0.12 pydantic-2.12.5 pydantic-core-2.41.5 smart-open-7.5.0 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 typer-slim-0.20.1 typing-inspection-0.4.2 wasabi-1.1.3 weasel-0.4.3 wrapt-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b0edc-8889-497f-8613-8e409666224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code import the spacy librabry and load the english language model 'en_core_web_sm' for nlp\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650bdabc-1dd2-4448-9b5f-4f080f5bca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(sent5)\n",
    "doc2 =nlp(sent6)\n",
    "doc3 =nlp(sent7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ca85d-1b66-4095-8697-deb0fcbf1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Token Genrated\n",
    "for token in doc2:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376f893-b8e0-432c-b9a2-241b200b4463",
   "metadata": {},
   "source": [
    "# 10. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a4a18-5444-4bed-b6b3-0610f302a82c",
   "metadata": {},
   "source": [
    "Stemming is a text preprocessing technique in NLP used to reduce words to their root or base form, known as a stem, by removing suffixes. It helps in simplifying the vocabulary and reducing word variations, thereby improving the efficiency of downstream NLP tasks like information retrieval and sentiment analysis. By converting words to their common root, stemming increases the overlap between related words, enhancing the generalization ability of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c9615562-e635-4168-b3e3-14ef97dca280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7a59b006-727a-4da5-a9e7-a07ad4268886",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "new_text = []\n",
    "\n",
    "def stem_words(text):\n",
    "    for i in text.split():\n",
    "        new_text = stemmer.stem(i)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dc473dce-187f-4add-bfcc-03e7cab6d1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w a l k'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = \"walk walks walking walked\"\n",
    "stem_words(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "913bc534-44b7-4b94-873d-71d338a6ccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy \n",
      "or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings\n",
      " tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like \n",
      " dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the \n",
      " world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'m o v i'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy \n",
    "or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings\n",
    " tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like \n",
    " dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the \n",
    " world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie\"\"\"\n",
    "print(text)\n",
    "\n",
    "# Calling Function\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6072effd-cc26-4422-aa96-fa3f0f0f530d",
   "metadata": {},
   "source": [
    "# 11. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069da0f-826a-4105-9e8d-cf10284f264a",
   "metadata": {},
   "source": [
    "Lemmatization is performed in NLP text preprocessing to reduce words to their base or dictionary form (lemma), enhancing consistency and simplifying analysis. Unlike stemming, which truncates words to their root form without considering meaning, lemmatization ensures that words are transformed to their canonical form, considering their part of speech. This process aids in reducing redundancy, improving text normalization, and enhancing the accuracy of downstream NLP tasks such as sentiment analysis, topic modeling, and information retrieval. Overall, lemmatization contributes to refining text data, facilitating more effective linguistic analysis and machine learning model performance.\n",
    "\n",
    "The code imports the WordNetLemmatizer from NLTK library and initializes it.\n",
    "It defines a sentence and a set of punctuation characters. The sentence is tokenized into words.\n",
    "Then, it iterates through each word in the sentence, removing punctuation if present.\n",
    "Next, it lemmatizes each word using the WordNetLemmatizer with a specific part-of-speech tag ('v' for verb).\n",
    "Finally, it prints each word along with its corresponding lemma after lemmatization, aligning them in a formatted table.\n",
    "This process helps to normalize the words in the sentence by reducing them to their base or dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6cf6d3c1-820b-45b1-80f0-076efb4ba395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "13817063-39f3-49c6-b9b9-d9f0039513e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "punc\n",
    "\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "\n",
    "# Using a Loop to Remove Punctuations.\n",
    "for word in sentence_words:\n",
    "    if word in punc:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea39f17b-226d-48b0-81f6-b9f3aa57a0f0",
   "metadata": {},
   "source": [
    "Well That's how the Lemmatizer Works.One Best Thing of Lemmatization is That, lemmatization ensures that words are transformed to their canonical form, considering their part of speech.However this Process is Slow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
